{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef070af0-3a62-48d5-b2a5-a5a8532340dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Punto 1\n",
    "\n",
    "# La lectura es la más rápida posible porque definimos schemas explícitos (evitando inferSchema y su escaneo previo), aprovechamos el particionamiento físico por anno_firma=YYYY de SECOP con basePath + recursiveFileLookup (permitiendo partition pruning cuando se filtra por año), y consumimos los formatos comprimidos nativos (.json.gz/.csv.gz) que Spark procesa en paralelo y distribuido directamente desde wasbs://, sin pasos intermedios ni cache innecesario. Esto reduce I/O, CPU de inferencia y tiempos totales de carga.\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "# Rutas\n",
    "SECOP_PATH = \"wasbs://sid@uniandesyjt.blob.core.windows.net/secop\"\n",
    "BPIN_PATH  = \"wasbs://sid@uniandesyjt.blob.core.windows.net/bpin/bpin.csv.gz\"\n",
    "\n",
    "# -------------------------\n",
    "# SCHEMA COMPLETO: SECOP\n",
    "# -------------------------\n",
    "secop_schema = T.StructType([\n",
    "    T.StructField(\"anno_bpin\", T.StringType(), True),\n",
    "    T.StructField(\"anno_firma\", T.LongType(), True),\n",
    "    T.StructField(\"ciudad\", T.StringType(), True),\n",
    "    T.StructField(\"código_bpin\", T.StringType(), True),\n",
    "    T.StructField(\"departamento\", T.StringType(), True),\n",
    "    T.StructField(\"documento_proveedor\", T.StringType(), True),\n",
    "    T.StructField(\"duración_del_contrato\", T.StringType(), True),\n",
    "    T.StructField(\"entidad_centralizada\", T.StringType(), True),\n",
    "    T.StructField(\"estado_contrato\", T.StringType(), True),\n",
    "    T.StructField(\"fecha_de_fin_del_contrato\", T.StringType(), True),\n",
    "    T.StructField(\"fecha_de_firma\", T.StringType(), True),\n",
    "    T.StructField(\"fecha_de_inicio_del_contrato\", T.StringType(), True),\n",
    "    T.StructField(\"modalidad_de_contratacion\", T.StringType(), True),\n",
    "    T.StructField(\"objeto_del_contrato\", T.StringType(), True),\n",
    "    T.StructField(\"orden\", T.StringType(), True),\n",
    "    T.StructField(\"origen_de_los_recursos\", T.StringType(), True),\n",
    "    T.StructField(\"proveedor_adjudicado\", T.StringType(), True),\n",
    "    T.StructField(\"rama\", T.StringType(), True),\n",
    "    T.StructField(\"sector\", T.StringType(), True),\n",
    "    T.StructField(\"tipo_de_contrato\", T.StringType(), True),\n",
    "    T.StructField(\"tipodocproveedor\", T.StringType(), True),\n",
    "    T.StructField(\"ultima_actualizacion\", T.StringType(), True),\n",
    "    T.StructField(\"urlproceso\", T.StringType(), True),\n",
    "    T.StructField(\"valor_del_contrato\", T.LongType(), True),\n",
    "    T.StructField(\"valor_pagado\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "# -------------------------\n",
    "# SCHEMA COMPLETO: BPIN\n",
    "# -------------------------\n",
    "bpin_schema = T.StructType([\n",
    "    T.StructField(\"Bpin\", T.StringType(), True),\n",
    "    T.StructField(\"NombreProyecto\", T.StringType(), True),\n",
    "    T.StructField(\"ObjetivoGeneral\", T.StringType(), True),\n",
    "    T.StructField(\"EstadoProyecto\", T.StringType(), True),\n",
    "    T.StructField(\"Horizonte\", T.StringType(), True),\n",
    "    T.StructField(\"Sector\", T.StringType(), True),\n",
    "    T.StructField(\"EntidadResponsable\", T.StringType(), True),\n",
    "    T.StructField(\"ProgramaPresupuestal\", T.StringType(), True),\n",
    "    T.StructField(\"TipoProyecto\", T.StringType(), True),\n",
    "    T.StructField(\"PlanDesarrolloNacional\", T.StringType(), True),\n",
    "    T.StructField(\"ValorTotalProyecto\", T.StringType(), True),\n",
    "    T.StructField(\"ValorVigenteProyecto\", T.StringType(), True),\n",
    "    T.StructField(\"ValorObligacionProyecto\", T.StringType(), True),\n",
    "    T.StructField(\"ValorPagoProyecto\", T.StringType(), True),\n",
    "    T.StructField(\"SubEstadoProyecto\", T.StringType(), True),\n",
    "    T.StructField(\"CodigoEntidadResponsable\", T.StringType(), True),\n",
    "    T.StructField(\"TotalBeneficiario\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "# --- Lectura SECOP (JSON.gz particionado por anno_firma=YYYY) ---\n",
    "secop_df = (spark.read\n",
    "    .schema(secop_schema)\n",
    "    .option(\"basePath\", SECOP_PATH)            # respeta particiones\n",
    "    .option(\"recursiveFileLookup\", \"true\")     # recorre subcarpetas anno_firma=YYYY\n",
    "    .json(SECOP_PATH)\n",
    ")\n",
    "\n",
    "# --- Lectura BPIN (CSV.gz con header) ---\n",
    "bpin_df = (spark.read\n",
    "    .schema(bpin_schema)\n",
    "    .option(\"header\", True)\n",
    "    .csv(BPIN_PATH)\n",
    ")\n",
    "\n",
    "# Quick checks\n",
    "display(secop_df.limit(10))\n",
    "display(bpin_df.limit(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eda3d42-bfdd-47ed-ba9a-54ccabfa5882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Punto 2\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Calcular el valor total de contratos por proveedor en 2024\n",
    "top_proveedores_2024 = (\n",
    "    secop_df\n",
    "    .filter(F.col(\"anno_firma\") == 2024)  # usamos partición por año\n",
    "    .groupBy(\"proveedor_adjudicado\")\n",
    "    .agg(F.sum(\"valor_del_contrato\").alias(\"valor_total_contratos\"))\n",
    "    .orderBy(F.desc(\"valor_total_contratos\"))\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "display(top_proveedores_2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caac48b9-6347-4941-95db-91b63ebf38a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Punto 3\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Normalización numérica robusta (valor_pagado viene como string)\n",
    "secop_num = (\n",
    "    secop_df\n",
    "    .withColumn(\"valor_contrato_num\", F.col(\"valor_del_contrato\").cast(\"double\"))\n",
    "    .withColumn(\n",
    "        \"valor_pagado_num\",\n",
    "        F.regexp_replace(F.col(\"valor_pagado\"), r\"[^0-9\\-.,]\", \"\")  # elimina caracteres no numéricos\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"valor_pagado_num\",\n",
    "        F.regexp_replace(F.col(\"valor_pagado_num\"), \",\", \"\")        # elimina comas de miles\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"valor_pagado_num\",\n",
    "        F.when(F.col(\"valor_pagado_num\") == \"\", None)\n",
    "         .otherwise(F.col(\"valor_pagado_num\").cast(\"double\"))\n",
    "    )\n",
    "    .withColumn(\"bpin\", F.upper(F.trim(F.col(\"código_bpin\"))))\n",
    ")\n",
    "\n",
    "bpin_norm = bpin_df.withColumn(\"bpin\", F.upper(F.trim(F.col(\"Bpin\"))))\n",
    "\n",
    "top_proyectos_sin_pagar = (\n",
    "    secop_num\n",
    "    .groupBy(\"bpin\")\n",
    "    .agg(\n",
    "        F.sum(F.col(\"valor_contrato_num\") - F.coalesce(F.col(\"valor_pagado_num\"), F.lit(0.0)))\n",
    "         .alias(\"valor_sin_pagar\")\n",
    "    )\n",
    "    # Aquí se aplica broadcast al lado pequeño (BPIN)\n",
    "    .join(broadcast(bpin_norm.select(\"bpin\", F.col(\"NombreProyecto\").alias(\"nombre_proyecto\"))),\n",
    "          on=\"bpin\", how=\"left\")\n",
    "    .orderBy(F.desc(\"valor_sin_pagar\"))\n",
    "    .limit(10)\n",
    "    .select(\"bpin\", \"nombre_proyecto\", \"valor_sin_pagar\")\n",
    ")\n",
    "display(top_proyectos_sin_pagar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb7e4c3-bfaa-4743-bba7-989681ec556b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Punto 4\n",
    "\n",
    "from pyspark.sql import Window, functions as F\n",
    "\n",
    "agregado = (\n",
    "    secop_df\n",
    "    .groupBy(\"anno_firma\", \"proveedor_adjudicado\")\n",
    "    .agg(F.sum(\"valor_del_contrato\").alias(\"valor_total\"))\n",
    ")\n",
    "\n",
    "w = Window.partitionBy(\"anno_firma\").orderBy(F.desc(\"valor_total\"))\n",
    "\n",
    "top5_por_ano = (\n",
    "    agregado\n",
    "    .withColumn(\"posicion\", F.row_number().over(w))\n",
    "    .filter(F.col(\"posicion\") <= 5)\n",
    "    .orderBy(\"anno_firma\", \"posicion\")\n",
    ")\n",
    "\n",
    "display(top5_por_ano)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24cf881b-e999-41a2-819e-02b403904e9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Punto 5\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "prov_2024 = (\n",
    "    secop_df\n",
    "    .filter(F.col(\"anno_firma\") == 2024)\n",
    "    .select(F.upper(F.trim(F.col(\"proveedor_adjudicado\"))).alias(\"proveedor\"))\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "prov_2020 = (\n",
    "    secop_df\n",
    "    .filter(F.col(\"anno_firma\") == 2020)\n",
    "    .select(F.upper(F.trim(F.col(\"proveedor_adjudicado\"))).alias(\"proveedor\"))\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "solo_2024 = prov_2024.join(prov_2020, on=\"proveedor\", how=\"left_anti\")\n",
    "\n",
    "# Conteo solicitado\n",
    "resultado_p5 = solo_2024.agg(F.count(\"*\").alias(\"proveedores_solo_2024\"))\n",
    "display(resultado_p5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8321fcd5-9549-4253-a30c-33272c7fbcf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Punto 6\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Filtramos SOLO lo necesario (column pruning) y usamos un filtro robusto por tipo de contrato.\n",
    "# Nota: valor_del_contrato ya es LongType según el schema cargado.\n",
    "servicios = (\n",
    "    secop_df\n",
    "    .select(\"tipo_de_contrato\", \"valor_del_contrato\")\n",
    "    .filter(F.lower(F.col(\"tipo_de_contrato\")).like(\"%prestación de servicios%\"))\n",
    ")\n",
    "\n",
    "# Promedio (exacto) y mediana aproximada (percentile_approx) para minimizar tiempo\n",
    "resultado_p6 = servicios.agg(\n",
    "    F.avg(\"valor_del_contrato\").alias(\"promedio_valor_contrato\"),\n",
    "    F.expr(\"percentile_approx(valor_del_contrato, 0.5)\").alias(\"mediana_aproximada\")\n",
    ")\n",
    "\n",
    "display(resultado_p6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c82330c-0cb1-4e39-ae81-cb244c7c59aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Punto 7\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Contamos proveedores distintos por año usando approx_count_distinct (rápido, tolera un pequeño error)\n",
    "prov_por_ano = (\n",
    "    secop_df\n",
    "    .select(\"anno_firma\", \"proveedor_adjudicado\")\n",
    "    .groupBy(\"anno_firma\")\n",
    "    .agg(F.approx_count_distinct(\"proveedor_adjudicado\", rsd=0.01).alias(\"proveedores_unicos_aprox\"))\n",
    "    .orderBy(F.desc(\"proveedores_unicos_aprox\"))\n",
    ")\n",
    "\n",
    "# Top 1 (año con más proveedores distintos)\n",
    "top_ano = prov_por_ano.limit(1)\n",
    "\n",
    "display(prov_por_ano)\n",
    "display(top_ano)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d9320b6-0960-4724-a7a6-4a761c9011ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Punto 8 - 20 palabras más comunes en el 20% superior de 2020\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) Umbral del 20% superior (percentil 80) para 2020 — rápido con percentile_approx\n",
    "p80_2020 = (\n",
    "    secop_df\n",
    "    .filter(F.col(\"anno_firma\") == 2020)\n",
    "    .agg(F.expr(\"percentile_approx(valor_del_contrato, 0.8)\").alias(\"p80\"))\n",
    "    .collect()[0][\"p80\"]\n",
    ")\n",
    "\n",
    "# 2) Filtrar solo el 20% superior por valor en 2020 y quedarnos con el objeto contractual\n",
    "top20_df = (\n",
    "    secop_df\n",
    "    .filter( (F.col(\"anno_firma\") == 2020) & (F.col(\"valor_del_contrato\") >= F.lit(p80_2020)) )\n",
    "    .select(F.coalesce(F.col(\"objeto_del_contrato\"), F.lit(\"\")).alias(\"objeto\"))\n",
    ")\n",
    "\n",
    "# 3) Normalizar texto: a minúsculas, quitar puntuación/números y tokenizar\n",
    "tokens_df = (\n",
    "    top20_df\n",
    "    .select(\n",
    "        F.split(\n",
    "            F.regexp_replace(F.lower(F.col(\"objeto\")), r\"[^a-záéíóúñü]+\", \" \"),\n",
    "            r\"\\s+\"\n",
    "        ).alias(\"words\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4) Remover stopwords (lista básica en español) sin usar StopWordsRemover.loadDefaultStopWords\n",
    "stopwords_es = [\n",
    "    \"a\",\"acá\",\"ahí\",\"al\",\"algo\",\"algún\",\"alguna\",\"algunas\",\"alguno\",\"algunos\",\"allá\",\"allí\",\"ante\",\"antes\",\n",
    "    \"aquel\",\"aquella\",\"aquellas\",\"aquello\",\"aquellos\",\"aquí\",\"así\",\"aun\",\"aún\",\"bajo\",\"bien\",\"cada\",\"casi\",\n",
    "    \"como\",\"con\",\"contra\",\"cual\",\"cuales\",\"cualquier\",\"cualquiera\",\"cuyos\",\"de\",\"del\",\"desde\",\"donde\",\"dos\",\n",
    "    \"el\",\"él\",\"ella\",\"ellas\",\"ello\",\"ellos\",\"en\",\"entre\",\"era\",\"erais\",\"eran\",\"eras\",\"eres\",\"es\",\"esa\",\"esas\",\n",
    "    \"ese\",\"eso\",\"esos\",\"esta\",\"estaba\",\"estaban\",\"estado\",\"estados\",\"estamos\",\"estar\",\"estará\",\"estas\",\"este\",\n",
    "    \"esto\",\"estos\",\"estoy\",\"fin\",\"fue\",\"fueron\",\"ha\",\"hace\",\"hacen\",\"hacer\",\"hacia\",\"han\",\"hasta\",\"hay\",\"la\",\n",
    "    \"las\",\"le\",\"les\",\"lo\",\"los\",\"más\",\"me\",\"mi\",\"mis\",\"mismo\",\"mismos\",\"muy\",\"nada\",\"ni\",\"no\",\"nos\",\"nosotros\",\n",
    "    \"nuestra\",\"nuestras\",\"nuestro\",\"nuestros\",\"o\",\"otra\",\"otras\",\"otro\",\"otros\",\"para\",\"pero\",\"poco\",\"por\",\n",
    "    \"porque\",\"que\",\"qué\",\"quien\",\"quién\",\"quienes\",\"quienesquiera\",\"se\",\"sea\",\"sean\",\"ser\",\"si\",\"sí\",\"siempre\",\n",
    "    \"sin\",\"sobre\",\"son\",\"su\",\"sus\",\"tal\",\"también\",\"tan\",\"tanto\",\"te\",\"tendrá\",\"ti\",\"tiene\",\"tienen\",\"toda\",\n",
    "    \"todas\",\"todavía\",\"todo\",\"todos\",\"tras\",\"tu\",\"tus\",\"un\",\"una\",\"unas\",\"uno\",\"unos\",\"usted\",\"ustedes\",\"ya\"\n",
    "]\n",
    "\n",
    "# Creamos una columna con el array de stopwords y filtramos con higher-order functions (sin ML)\n",
    "filtered_df = (\n",
    "    tokens_df\n",
    "    .withColumn(\"stop_es\", F.array([F.lit(w) for w in stopwords_es]))\n",
    "    .select(F.expr(\"filter(words, w -> NOT array_contains(stop_es, w))\").alias(\"filtered\"))\n",
    ")\n",
    "\n",
    "# 5) Contar frecuencia y mostrar TOP 20\n",
    "top_words = (\n",
    "    filtered_df\n",
    "    .select(F.explode(\"filtered\").alias(\"palabra\"))\n",
    "    .filter(F.length(\"palabra\") > 1)\n",
    "    .groupBy(\"palabra\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "    .limit(20)\n",
    ")\n",
    "\n",
    "display(top_words)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Proyecto Parte 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
